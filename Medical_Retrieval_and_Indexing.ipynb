{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 0. Environment Configuration & Dependency Installation\n","**Objective:** Prepare the runtime environment by installing necessary external tools and linking storage.\n","\n","**Context:**\n","This project requires specific libraries that are not pre-installed in Google Colab:\n","1.  **SpaCy (`fr_core_news_sm`):** A natural language processing model required for tokenizing French text and removing stop words during the indexing phase.\n","2.  **Percollate:** A Node.js command-line tool used to transform HTML web pages into clean, readable Markdown files.\n","3.  **Google Drive:** Mounted to persist our data (`meta_data.json`, `index_inverse.json`) and document corpus, ensuring work is saved between sessions."],"metadata":{"id":"j1scqjkDxWWa"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEUlewRwC48l","outputId":"d88214e3-6a14-4c74-a0ec-eb96e9765074","collapsed":true,"executionInfo":{"status":"ok","timestamp":1765447357077,"user_tz":-60,"elapsed":41353,"user":{"displayName":"HAYAT IMHAH","userId":"08898796348888468275"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fr-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m source-map-resolve@0.6.0: See https://github.com/lydell/source-map-resolve#deprecated\n","\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m abab@2.0.6: Use your platform's native atob() and btoa() methods instead\n","\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m domexception@4.0.0: Use your platform's native DOMException instead\n","\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m node-domexception@1.0.0: Use your platform's native DOMException instead\n","\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m puppeteer@19.11.1: < 24.15.0 is no longer supported\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n","changed 349 packages in 22s\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K117 packages are looking for funding\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K  run `npm fund` for details\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0KDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Working Directory set to: /content/drive/MyDrive/SRI-Medical-Search\n"]}],"source":["# --- 1. SETUP ENVIRONMENT ---\n","# Install the necessary NLP model for French\n","!pip install -q spacy\n","!python -m spacy download fr_core_news_sm\n","\n","# Install 'percollate' (Node.js tool) used in your retrieval script\n","# This is required to convert web pages to Markdown\n","!npm install -g percollate\n","\n","# Mount Google Drive to access your project files\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","# --- CONFIGURATION ---\n","# UPDATE THIS PATH to your actual project folder in Drive\n","PROJECT_PATH = \"/content/drive/MyDrive/SRI-Medical-Search\"\n","\n","# Navigate to project folder\n","os.chdir(PROJECT_PATH)\n","print(f\"Working Directory set to: {os.getcwd()}\")"]},{"cell_type":"markdown","source":["## 1. Preprocessing: ID Assignment\n","**Objective:** Assign a unique ID to each medication in our `meta_data.json` source file.\n","\n","**Context:**\n","We start with a manually collected list of Name/URL pairs. This step adds a unique `\"id\": X` field to each entry, which acts as the **primary key** for our search engine.\n","\n","**Why this is necessary:**\n","1.  **Linkage:** Establishes a robust link between the metadata (JSON) and the downloaded document files (e.g., `1.md`, `2.md`).\n","2.  **Consistency:** Avoids encoding issues associated with using complex file names (e.g., special characters in medication names).\n","3.  **Frontend Integration:** Facilitates efficient metadata retrieval for the User Interface. The frontend can instantly fetch details (like the official name or source URL) using this ID without parsing the full document text."],"metadata":{"id":"L5pUH3dLwhXd"}},{"cell_type":"code","source":["import json\n","import os\n","\n","# --- CONFIGURATION ---\n","JSON_FILE = \"meta_data.json\"\n","\n","def assign_unique_ids(file_path):\n","    \"\"\"\n","    Reads the metadata JSON file, assigns a unique sequential ID to each entry\n","    if it does not already exist, and overwrites the file in place.\n","    \"\"\"\n","    if not os.path.exists(file_path):\n","        print(f\"[ERROR] File not found: {file_path}\")\n","        return\n","\n","    try:\n","        # Load existing data\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        print(f\"[INFO] Loaded {len(data)} entries.\")\n","\n","        updated = False\n","\n","        # Iterate and assign IDs\n","        for index, entry in enumerate(data, start=1):\n","            if 'id' not in entry:\n","                # Assign simple integer ID based on current list position\n","                # Using 'insert' ensures 'id' is the first key in the visual JSON\n","                original_data = entry.copy()\n","                entry.clear()\n","                entry['id'] = index\n","                entry.update(original_data)\n","                updated = True\n","\n","        # Write back to file only if changes were made\n","        if updated:\n","            with open(file_path, 'w', encoding='utf-8') as f:\n","                json.dump(data, f, indent=4, ensure_ascii=False)\n","            print(f\"[SUCCESS] IDs added and file updated: {file_path}\")\n","        else:\n","            print(\"[INFO] All entries already have IDs. No changes needed.\")\n","\n","    except json.JSONDecodeError:\n","        print(f\"[ERROR] Failed to decode JSON from {file_path}.\")\n","\n","# Execute the function\n","assign_unique_ids(JSON_FILE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vT3qEd0Iuc_h","executionInfo":{"status":"ok","timestamp":1765447357103,"user_tz":-60,"elapsed":18,"user":{"displayName":"HAYAT IMHAH","userId":"08898796348888468275"}},"outputId":"fecc3ec3-8228-4768-cde5-48f74d60d3f3"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Loaded 50 entries.\n","[SUCCESS] IDs added and file updated: meta_data.json\n"]}]},{"cell_type":"markdown","source":["## 2. Data Acquisition: Automated Retrieval\n","**Objective:** Automated fetching of medical notices and conversion to a standardized Markdown format.\n","\n","**Context:**\n","This script iterates through the normalized `meta_data.json` file. For each entry, it extracts the unique `id` and source `url`. It then triggers the `percollate` tool to scrape the web page content and save it as a structured Markdown file (e.g., `med_md/1.md`).\n","\n","**Why this is necessary:**\n","* **Standardization:** Converting HTML to Markdown removes web clutter (navbars, ads), leaving only the relevant text for indexing.\n","* **Data Integrity:** Using the `id` for filenames ensures a strict 1-to-1 mapping between our metadata and our document corpus.\n","* **Efficiency:** Automating this process allows us to scale the corpus easily without manual copy-pasting."],"metadata":{"id":"cCYPTHDMxwEh"}},{"cell_type":"code","source":["import os\n","import subprocess\n","import time\n","import json\n","\n","# --- CONFIGURATION ---\n","JSON_SOURCE_FILE = \"meta_data.json\"\n","OUTPUT_DIR = \"med_md\"  # Folder for markdown files\n","\n","# Ensure output directory exists\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","def generate_markdown(doc_id, url):\n","    \"\"\"\n","    Generates a Markdown file from a URL using the 'percollate' tool.\n","    The filename is based on the document ID.\n","    \"\"\"\n","    filename = f\"{doc_id}.md\"\n","    output_path = os.path.join(OUTPUT_DIR, filename)\n","\n","    # Command to run percollate (outputting as markdown)\n","    # Note: 'percollate' must be installed in the environment\n","    cmd = [\"percollate\", \"md\", \"--output\", output_path, url]\n","\n","    try:\n","        print(f\"[PROCESSING] ID: {doc_id} -> {filename}...\")\n","\n","        # Execute the command\n","        result = subprocess.run(cmd, capture_output=True, text=True)\n","\n","        if result.returncode == 0:\n","            print(f\"    [SUCCESS] Saved to {output_path}\")\n","        else:\n","            print(f\"    [ERROR] Percollate failed: {result.stderr.strip()}\")\n","\n","        # Short pause to be polite to the server\n","        time.sleep(1)\n","\n","    except FileNotFoundError:\n","        print(\"    [CRITICAL] 'percollate' tool not found. Please run '!npm install -g percollate' first.\")\n","\n","def run_retrieval_process(json_file):\n","    \"\"\"\n","    Main function to iterate through the JSON and fetch data.\n","    \"\"\"\n","    if not os.path.exists(json_file):\n","        print(f\"[ERROR] JSON file not found: {json_file}\")\n","        return\n","\n","    with open(json_file, 'r', encoding='utf-8') as f:\n","        medicaments_list = json.load(f)\n","\n","    print(f\"[INFO] Starting retrieval for {len(medicaments_list)} items.\\n\")\n","\n","    for item in medicaments_list:\n","        # Extract ID and URL directly from the JSON object\n","        doc_id = item.get(\"id\")\n","        url = item.get(\"url\")\n","\n","        # Only proceed if both ID and URL are valid\n","        if doc_id and url:\n","            generate_markdown(doc_id, url)\n","        else:\n","            print(f\"[WARNING] Skipped entry (Missing ID or URL): {item}\")\n","\n","    print(f\"\\n[DONE] Retrieval complete. Check the '{OUTPUT_DIR}' folder.\")\n","\n","# Execute retrieval\n","run_retrieval_process(JSON_SOURCE_FILE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hnTpD9rUuw3N","executionInfo":{"status":"ok","timestamp":1765447632483,"user_tz":-60,"elapsed":275377,"user":{"displayName":"HAYAT IMHAH","userId":"08898796348888468275"}},"outputId":"18f70e3b-e4b1-47ff-c873-fa60d1199499"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Starting retrieval for 50 items.\n","\n","[PROCESSING] ID: 1 -> 1.md...\n","    [SUCCESS] Saved to med_md/1.md\n","[PROCESSING] ID: 2 -> 2.md...\n","    [SUCCESS] Saved to med_md/2.md\n","[PROCESSING] ID: 3 -> 3.md...\n","    [SUCCESS] Saved to med_md/3.md\n","[PROCESSING] ID: 4 -> 4.md...\n","    [SUCCESS] Saved to med_md/4.md\n","[PROCESSING] ID: 5 -> 5.md...\n","    [SUCCESS] Saved to med_md/5.md\n","[PROCESSING] ID: 6 -> 6.md...\n","    [SUCCESS] Saved to med_md/6.md\n","[PROCESSING] ID: 7 -> 7.md...\n","    [SUCCESS] Saved to med_md/7.md\n","[PROCESSING] ID: 8 -> 8.md...\n","    [SUCCESS] Saved to med_md/8.md\n","[PROCESSING] ID: 9 -> 9.md...\n","    [SUCCESS] Saved to med_md/9.md\n","[PROCESSING] ID: 10 -> 10.md...\n","    [SUCCESS] Saved to med_md/10.md\n","[PROCESSING] ID: 11 -> 11.md...\n","    [SUCCESS] Saved to med_md/11.md\n","[PROCESSING] ID: 12 -> 12.md...\n","    [SUCCESS] Saved to med_md/12.md\n","[PROCESSING] ID: 13 -> 13.md...\n","    [SUCCESS] Saved to med_md/13.md\n","[PROCESSING] ID: 14 -> 14.md...\n","    [SUCCESS] Saved to med_md/14.md\n","[PROCESSING] ID: 15 -> 15.md...\n","    [SUCCESS] Saved to med_md/15.md\n","[PROCESSING] ID: 16 -> 16.md...\n","    [SUCCESS] Saved to med_md/16.md\n","[PROCESSING] ID: 17 -> 17.md...\n","    [SUCCESS] Saved to med_md/17.md\n","[PROCESSING] ID: 18 -> 18.md...\n","    [SUCCESS] Saved to med_md/18.md\n","[PROCESSING] ID: 19 -> 19.md...\n","    [SUCCESS] Saved to med_md/19.md\n","[PROCESSING] ID: 20 -> 20.md...\n","    [SUCCESS] Saved to med_md/20.md\n","[PROCESSING] ID: 21 -> 21.md...\n","    [SUCCESS] Saved to med_md/21.md\n","[PROCESSING] ID: 22 -> 22.md...\n","    [SUCCESS] Saved to med_md/22.md\n","[PROCESSING] ID: 23 -> 23.md...\n","    [SUCCESS] Saved to med_md/23.md\n","[PROCESSING] ID: 24 -> 24.md...\n","    [SUCCESS] Saved to med_md/24.md\n","[PROCESSING] ID: 25 -> 25.md...\n","    [SUCCESS] Saved to med_md/25.md\n","[PROCESSING] ID: 26 -> 26.md...\n","    [SUCCESS] Saved to med_md/26.md\n","[PROCESSING] ID: 27 -> 27.md...\n","    [SUCCESS] Saved to med_md/27.md\n","[PROCESSING] ID: 28 -> 28.md...\n","    [SUCCESS] Saved to med_md/28.md\n","[PROCESSING] ID: 29 -> 29.md...\n","    [SUCCESS] Saved to med_md/29.md\n","[PROCESSING] ID: 30 -> 30.md...\n","    [SUCCESS] Saved to med_md/30.md\n","[PROCESSING] ID: 31 -> 31.md...\n","    [SUCCESS] Saved to med_md/31.md\n","[PROCESSING] ID: 32 -> 32.md...\n","    [SUCCESS] Saved to med_md/32.md\n","[PROCESSING] ID: 33 -> 33.md...\n","    [SUCCESS] Saved to med_md/33.md\n","[PROCESSING] ID: 34 -> 34.md...\n","    [SUCCESS] Saved to med_md/34.md\n","[PROCESSING] ID: 35 -> 35.md...\n","    [SUCCESS] Saved to med_md/35.md\n","[PROCESSING] ID: 36 -> 36.md...\n","    [SUCCESS] Saved to med_md/36.md\n","[PROCESSING] ID: 37 -> 37.md...\n","    [SUCCESS] Saved to med_md/37.md\n","[PROCESSING] ID: 38 -> 38.md...\n","    [SUCCESS] Saved to med_md/38.md\n","[PROCESSING] ID: 39 -> 39.md...\n","    [SUCCESS] Saved to med_md/39.md\n","[PROCESSING] ID: 40 -> 40.md...\n","    [SUCCESS] Saved to med_md/40.md\n","[PROCESSING] ID: 41 -> 41.md...\n","    [SUCCESS] Saved to med_md/41.md\n","[PROCESSING] ID: 42 -> 42.md...\n","    [SUCCESS] Saved to med_md/42.md\n","[PROCESSING] ID: 43 -> 43.md...\n","    [SUCCESS] Saved to med_md/43.md\n","[PROCESSING] ID: 44 -> 44.md...\n","    [SUCCESS] Saved to med_md/44.md\n","[PROCESSING] ID: 45 -> 45.md...\n","    [SUCCESS] Saved to med_md/45.md\n","[PROCESSING] ID: 46 -> 46.md...\n","    [SUCCESS] Saved to med_md/46.md\n","[PROCESSING] ID: 47 -> 47.md...\n","    [SUCCESS] Saved to med_md/47.md\n","[PROCESSING] ID: 48 -> 48.md...\n","    [SUCCESS] Saved to med_md/48.md\n","[PROCESSING] ID: 49 -> 49.md...\n","    [SUCCESS] Saved to med_md/49.md\n","[PROCESSING] ID: 50 -> 50.md...\n","    [SUCCESS] Saved to med_md/50.md\n","\n","[DONE] Retrieval complete. Check the 'med_md' folder.\n"]}]},{"cell_type":"markdown","source":["## 3. Core Engine: Parsing & Inverted Index Construction\n","**Objective:** Parse the raw Markdown documents to extract structured data and build the Inverted Index for the search engine.\n","\n","**Context:**\n","This phase transforms our collection of unstructured text files into a searchable data structure. We employ a **hybrid extraction strategy**:\n","1.  **Regex Extraction:** We use Regular Expressions to extract structural fields where precision is critical (e.g., *Name*, *Active Substance*).\n","2.  **NLP Pipeline:** We use SpaCy to process the descriptive text (*Indications*, *Posology*). This involves tokenization, stop-word removal, and lemmatization to improve search recall.\n","\n","**Output:**\n","The script generates `index_inverse.json`, a dictionary mapping every unique word to the list of documents containing it (including term frequency and positions). This is the fundamental data structure used by the search algorithm."],"metadata":{"id":"oWJt0W4hx1cK"}},{"cell_type":"code","source":["import os\n","import json\n","import re\n","import spacy\n","\n","# --- CONFIGURATION ---\n","DOCS_FOLDER = \"med_md\"\n","INDEX_JSON_PATH = \"index_inverse.json\"\n","METADATA_JSON_PATH = \"meta_data.json\"\n","\n","# --- LOAD NLP MODEL ---\n","try:\n","    nlp = spacy.load(\"fr_core_news_sm\")\n","    # Define custom stop words specific to medical notices\n","    CUSTOM_STOP_WORDS = {\"mg\", \"ml\", \"comprimé\", \"sachet\", \"gélule\", \"boîte\", \"notice\", \"médicament\", \"voir\", \"rubrique\", \"comprimer\", \"fois\", \"cas\", \"base\", \"substance\", \"active\"}\n","    STOP_WORDS = nlp.Defaults.stop_words\n","    ALL_STOP_WORDS = STOP_WORDS.union(CUSTOM_STOP_WORDS)\n","    print(\"[INFO] SpaCy model loaded successfully.\")\n","except Exception as e:\n","    print(f\"[ERROR] SpaCy model loading failed: {e}\")\n","\n","def clean_markdown_syntax(text):\n","    \"\"\"\n","    Cleans up Markdown syntax to simplify regex extraction.\n","    Removes links and bold markers.\n","    \"\"\"\n","    # Remove links: [Text](link) -> Text\n","    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n","    # Remove anchors: [](#top) -> \"\"\n","    text = re.sub(r'\\[\\]\\(#.*?\\)', '', text)\n","    # Remove bold/italic: **Text** -> Text\n","    text = re.sub(r'[*_]{2,}(.*?)[*_]{2,}', r'\\1', text)\n","    # Normalize newlines\n","    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n","    return text\n","\n","def parse_file_for_indexing(content):\n","    \"\"\"\n","    Parses a Markdown file and extracts key sections.\n","    Returns: Name, Molecule, Tech_Block, Desc_Block\n","    \"\"\"\n","    # 1. Clean Markdown\n","    content = clean_markdown_syntax(content)\n","\n","    # 2. Extract Name (Title)\n","    # Handles \"1. DENOMINATION...\" or simple titles\n","    regex_nom = r'(?:1\\.\\s*)?Dénomination du médicament.*?\\n+(.+)'\n","    match_nom = re.search(regex_nom, content, re.IGNORECASE)\n","\n","    if match_nom:\n","        nom = match_nom.group(1).strip()\n","    else:\n","        # Fallback: Look for uppercase title with dosage\n","        regex_fallback = r'\\n([A-Z\\s\\-\\(\\)]+\\d+[.,]?\\d*\\s?(?:mg|g|ml|%)[^\\n]+)'\n","        match_nom = re.search(regex_fallback, content)\n","        nom = match_nom.group(1).strip() if match_nom else \"Inconnu\"\n","\n","    nom = re.sub(r'[\\[\\]]', '', nom).strip()\n","\n","    # 3. Extract Molecule\n","    molecule = \"\"\n","    regex_molecule = r'(?:La substance active est\\s*:|Substance active\\s*:)\\s*\\n+(.+?)(?=\\n|Pour un)'\n","    match_molecule = re.search(regex_molecule, content, re.IGNORECASE)\n","\n","    if match_molecule:\n","        raw_molecule = match_molecule.group(1).strip()\n","        molecule = re.sub(r'\\.{2,}', ' ', raw_molecule).strip()\n","\n","    # 4. Extract Description Sections\n","    regex_indic = r'(?:DANS QUELS CAS EST-IL UTILISE|Indications thérapeutiques)(.+?)(?=\\n2\\.|QUELLES SONT)'\n","    match_indic = re.search(regex_indic, content, re.IGNORECASE | re.DOTALL)\n","    indic = match_indic.group(1).strip() if match_indic else \"\"\n","\n","    regex_poso = r'(?:Posologie|Mode d\\'administration)(.+?)(?=\\n(?:4\\.|QUELS SONT|Si vous avez pris))'\n","    match_poso = re.search(regex_poso, content, re.IGNORECASE | re.DOTALL)\n","    poso = match_poso.group(1).strip() if match_poso else \"\"\n","\n","    regex_effets = r'(?:QUELS SONT LES EFFETS|Effets indésirables)(.+?)(?=\\n(?:5\\.|COMMENT CONSERVER|Déclaration))'\n","    match_effets = re.search(regex_effets, content, re.IGNORECASE | re.DOTALL)\n","    effets = match_effets.group(1).strip() if match_effets else \"\"\n","\n","    # Prepare blocks\n","    bloc_technique = f\"{nom} {molecule}\"\n","    bloc_descriptif = f\"{indic} {poso} {effets}\"\n","    bloc_descriptif = re.sub(r'\\s+', ' ', bloc_descriptif).strip()\n","\n","    return nom, molecule, bloc_technique, bloc_descriptif\n","\n","\n","def tokenize_and_process(text, use_lemmatization=True):\n","    \"\"\"\n","    Tokenizes text, removes stop words, and optionally lemmatizes.\n","    \"\"\"\n","    doc = nlp(text)\n","    tokens = []\n","    for token in doc:\n","        # Lemma if requested, else exact text\n","        term = token.lemma_.lower() if use_lemmatization else token.text.lower()\n","        if token.is_alpha and term not in ALL_STOP_WORDS:\n","            tokens.append(term)\n","    return tokens\n","\n","def build_index():\n","    print(f\"[START] Starting indexing process...\")\n","\n","    if not os.path.exists(METADATA_JSON_PATH):\n","        print(f\"[ERROR] Metadata file missing: {METADATA_JSON_PATH}\")\n","        return\n","\n","    with open(METADATA_JSON_PATH, 'r', encoding='utf-8') as f:\n","        metadata_list = json.load(f)\n","\n","    inverted_index = {}\n","    count_indexed = 0\n","    total_docs = len(metadata_list)\n","\n","    for i, entry in enumerate(metadata_list, start=1):\n","        doc_id = entry.get('id')\n","        file_name = f\"{doc_id}.md\"\n","        file_path = os.path.join(DOCS_FOLDER, file_name)\n","\n","        print(f\"[{i}/{total_docs}] Processing ID: {doc_id} ...\", end=\" \")\n","\n","        if not os.path.exists(file_path):\n","            print(f\"[MISSING] ID {doc_id} (File {file_name} not found)\")\n","            continue\n","\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                content = f.read()\n","\n","            nom, molecule, tech_block, desc_block = parse_file_for_indexing(content)\n","\n","            # Update Metadata with extracted info\n","            entry['nom'] = nom\n","            entry['snippet'] = desc_block[:200] + \"...\" if desc_block else \"\"\n","\n","            # 1. Process Technical terms (No Lemmatization usually better for proper nouns, but user choice)\n","            tokens_tech = tokenize_and_process(tech_block, use_lemmatization=False)\n","            # 2. Process Descriptive terms (Lemmatization active)\n","            tokens_desc = tokenize_and_process(desc_block, use_lemmatization=True)\n","\n","            all_tokens = tokens_tech + tokens_desc\n","\n","            # Build Inverted Index\n","            for pos, token in enumerate(all_tokens):\n","                if token not in inverted_index:\n","                    inverted_index[token] = []\n","\n","                # Check if doc exists in list\n","                doc_entry = next((item for item in inverted_index[token] if item['doc_id'] == doc_id), None)\n","\n","                if doc_entry is None:\n","                    inverted_index[token].append({'doc_id': doc_id, 'tf': 1, 'positions': [pos]})\n","                else:\n","                    doc_entry['tf'] += 1\n","                    doc_entry['positions'].append(pos)\n","\n","            count_indexed += 1\n","            print(f\"-> [OK] Indexed '{nom}' ({len(all_tokens)} tokens)\")\n","\n","        except Exception as e:\n","            print(f\"[ERROR] Failed to index ID {doc_id}: {e}\")\n","\n","    # Save Results\n","    print(f\"\\n[SAVING] Writing index to disk...\")\n","    with open(INDEX_JSON_PATH, 'w', encoding='utf-8') as f:\n","        json.dump(inverted_index, f, indent=2, ensure_ascii=False)\n","\n","    with open(METADATA_JSON_PATH, 'w', encoding='utf-8') as f:\n","        json.dump(metadata_list, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\n[SUCCESS] Indexed {count_indexed} documents.\")\n","    print(f\"[INFO] Index saved to {INDEX_JSON_PATH}\")\n","\n","# Run Indexing\n","build_index()"],"metadata":{"id":"j_b5x_wZwaIN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765447910743,"user_tz":-60,"elapsed":25528,"user":{"displayName":"HAYAT IMHAH","userId":"08898796348888468275"}},"outputId":"6ce8f358-3421-49a2-ca5d-3f9e156aa409"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] SpaCy model loaded successfully.\n","[START] Starting indexing process...\n","[1/50] Processing ID: 1 ... -> [OK] Indexed 'AMOXICILLINE ACIDE CLAVULANIQUE ALMUS 100 mg/12,5 mg par mL ENFANTS, poudre pour suspension buvable (rapport amoxicilline/acide clavulanique : 8/1)' (555 tokens)\n","[2/50] Processing ID: 2 ... -> [OK] Indexed 'AUGMENTIN 1 g/125 mg, poudre pour suspension buvable en sachet-dose (rapport amoxicilline/acide clavulanique : 8/1)' (555 tokens)\n","[3/50] Processing ID: 3 ... -> [OK] Indexed 'CLAMOXYL 1 g, comprimé dispersible' (1347 tokens)\n","[4/50] Processing ID: 4 ... -> [OK] Indexed 'ORELOX 100 mg, comprimé pelliculé' (300 tokens)\n","[5/50] Processing ID: 5 ... -> [OK] Indexed 'PYOSTACINE 250 mg, comprimé pelliculé' (909 tokens)\n","[6/50] Processing ID: 6 ... -> [OK] Indexed 'ZITHROMAX 250 mg, comprimé pelliculé' (1001 tokens)\n","[7/50] Processing ID: 7 ... -> [OK] Indexed 'MONURIL 3 g, granulés pour solution buvable en sachet' (600 tokens)\n","[8/50] Processing ID: 8 ... -> [OK] Indexed 'FUCIDINE 2 POUR CENT, crème' (329 tokens)\n","[9/50] Processing ID: 9 ... -> [OK] Indexed 'FLAGYL 0,5 POUR CENT, solution injectable pour perfusion en poche' (1407 tokens)\n","[10/50] Processing ID: 10 ... -> [OK] Indexed 'ADVIL 200 mg, comprimé enrobé' (2201 tokens)\n","[11/50] Processing ID: 11 ... -> [OK] Indexed 'NUROFEN 200 mg, comprimé enrobé' (2254 tokens)\n","[12/50] Processing ID: 12 ... -> [OK] Indexed 'ANTARENE 100 mg, comprimé pelliculé' (1733 tokens)\n","[13/50] Processing ID: 13 ... -> [OK] Indexed 'VOLTARENE 100 mg, suppositoire' (2773 tokens)\n","[14/50] Processing ID: 14 ... -> [OK] Indexed 'APRANAX 275 mg, comprimé pelliculé' (1926 tokens)\n","[15/50] Processing ID: 15 ... -> [OK] Indexed '40 gouttes correspondent à 1 ml de solution et à 0,5 mg de bétaméthasone.' (397 tokens)\n","[16/50] Processing ID: 16 ... -> [OK] Indexed 'SOLUPRED 20 mg, comprimé orodispersible' (1266 tokens)\n","[17/50] Processing ID: 17 ... -> [OK] Indexed 'CORTANCYL 1 mg, comprimé' (1245 tokens)\n","[18/50] Processing ID: 18 ... -> [OK] Indexed 'KETUM 100 mg, comprimé pelliculé' (1923 tokens)\n","[19/50] Processing ID: 19 ... -> [OK] Indexed 'SMECTA 3 g FRAISE, poudre pour suspension buvable en sachet' (602 tokens)\n","[20/50] Processing ID: 20 ... -> [OK] Indexed 'IMODIUM 0,2 mg/mL ENFANTS, solution buvable' (1509 tokens)\n","[21/50] Processing ID: 21 ... -> [OK] Indexed 'VOGALENE 0,1 %, solution buvable en flacon' (902 tokens)\n","[22/50] Processing ID: 22 ... -> [OK] Indexed 'MOTILIUM 1 mg/ml, suspension buvable' (390 tokens)\n","[23/50] Processing ID: 23 ... -> [OK] Indexed 'GAVISCON MENTHE, comprimé à croquer' (399 tokens)\n","[24/50] Processing ID: 24 ... -> [OK] Indexed 'DEBRIDAT 100 mg, comprimé pelliculé' (292 tokens)\n","[25/50] Processing ID: 25 ... -> [OK] Indexed 'SPASFON LYOC 160 mg, lyophilisat oral' (421 tokens)\n","[26/50] Processing ID: 26 ... -> [OK] Indexed 'MAALOX MAUX D’ESTOMAC HYDROXYDE D’ALUMINIUM/ HYDROXYDE DE MAGNESIUM 400 mg/ 400 mg SANS SUCRE FRUITS ROUGES, comprimé à croquer édulcoré à la saccharine sodique, au sorbitol et au maltitol' (1054 tokens)\n","[27/50] Processing ID: 27 ... -> [OK] Indexed 'Inconnu' (544 tokens)\n","[28/50] Processing ID: 28 ... -> [OK] Indexed 'MOPRAL 10 mg, gélule gastro-résistante' (981 tokens)\n","[29/50] Processing ID: 29 ... -> [OK] Indexed 'INEXIUM 10 mg, granulés gastro-résistants pour suspension buvable en sachet' (554 tokens)\n","[30/50] Processing ID: 30 ... -> [OK] Indexed 'METEOSPASMYL, capsule molle' (371 tokens)\n","[31/50] Processing ID: 31 ... -> [OK] Indexed 'DEBRIDAT 100 mg, comprimé pelliculé' (292 tokens)\n","[32/50] Processing ID: 32 ... -> [OK] Indexed 'BEDELIX, poudre pour suspension buvable en sachet' (380 tokens)\n","[33/50] Processing ID: 33 ... -> [OK] Indexed 'EFFERALGAN 1000 mg, comprimé effervescent' (375 tokens)\n","[34/50] Processing ID: 34 ... -> [OK] Indexed 'DAFALGAN 1000 mg, comprimé effervescent' (1667 tokens)\n","[35/50] Processing ID: 35 ... -> [OK] Indexed 'FERVEX ADULTES FRAMBOISE, granulés pour solution buvable en sachet' (1531 tokens)\n","[36/50] Processing ID: 36 ... -> [OK] Indexed 'IXPRIM 37,5 mg/325 mg, comprimé effervescent' (1742 tokens)\n","[37/50] Processing ID: 37 ... -> [OK] Indexed '8 gélules sous plaquettes thermoformées (PVC/Aluminium)' (923 tokens)\n","[38/50] Processing ID: 38 ... -> [OK] Indexed 'TOPALGIC 100 mg/2 ml, solution injectable en ampoule' (1375 tokens)\n","[39/50] Processing ID: 39 ... -> [OK] Indexed 'ZYRTEC 10 mg/ml, solution buvable en gouttes' (653 tokens)\n","[40/50] Processing ID: 40 ... -> [OK] Indexed 'CLARITYNE 10 mg, comprimé' (559 tokens)\n","[41/50] Processing ID: 41 ... -> [OK] Indexed 'TOPLEXIL 0,33 mg/ml, SANS SUCRE, solution buvable édulcorée à l’acésulfame potassique.' (1542 tokens)\n","[42/50] Processing ID: 42 ... -> [OK] Indexed 'HELICIDINE 10 POUR CENT SANS SUCRE, sirop édulcoré à la saccharine sodique et au maltitol liquide' (570 tokens)\n","[43/50] Processing ID: 43 ... -> [OK] Indexed 'MUCOMYST 200 mg, poudre pour solution buvable en sachet' (505 tokens)\n","[44/50] Processing ID: 44 ... -> [OK] Indexed 'EXOMUC 200 mg, granulés pour solution buvable en sachet' (663 tokens)\n","[45/50] Processing ID: 45 ... -> [OK] Indexed 'VENTOLINE 0,5 mg/1 ml, solution injectable par voie sous-cutanée en ampoule.' (400 tokens)\n","[46/50] Processing ID: 46 ... -> [OK] Indexed 'Inconnu' (2140 tokens)\n","[47/50] Processing ID: 47 ... -> [OK] Indexed 'HUMEX RHUME DES FOINS A LA BECLOMETASONE 50 microgrammes/dose, suspension pour pulvérisation nasale en flacon' (833 tokens)\n","[48/50] Processing ID: 48 ... -> [OK] Indexed 'KARDEGIC 160 mg, poudre pour solution buvable en sachet' (437 tokens)\n","[49/50] Processing ID: 49 ... -> [OK] Indexed 'TAHOR 10 mg, comprimé à croquer' (825 tokens)\n","[50/50] Processing ID: 50 ... -> [OK] Indexed 'CARDENSIEL 1,25 mg, comprimé pelliculé' (496 tokens)\n","\n","[SAVING] Writing index to disk...\n","\n","[SUCCESS] Indexed 50 documents.\n","[INFO] Index saved to index_inverse.json\n"]}]}]}